import os
import re
import socket
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_openai import OpenAIEmbeddings

# =====================================================
# ‚öôÔ∏è Basic setup
# =====================================================
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["LANGCHAIN_DISABLE_TELEMETRY"] = "true"
os.environ["CHROMA_TELEMETRY_ENABLED"] = "false"
os.environ["POSTHOG_DISABLED"] = "true"

data_folder = "data"
persist_dir = "engine_db"

# =====================================================
# üîç Internet check
# =====================================================
def check_internet(host="8.8.8.8", port=53, timeout=2):
    try:
        socket.setdefaulttimeout(timeout)
        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))
        return True
    except socket.error:
        return False


# =====================================================
# üß† Choose embedding mode
# =====================================================
if check_internet():
    print("üåê Internet detected ‚Äî using OpenAI embeddings (fast & large context).")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    chunk_size = 2000
    chunk_overlap = 150
else:
    print("üß† Offline mode ‚Äî using local Ollama embeddings.")
    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    chunk_size = 600
    chunk_overlap = 80

print(f"ü™∂ Chunk size: {chunk_size}, overlap: {chunk_overlap}")

# =====================================================
# üìò Load PDFs
# =====================================================
all_docs = []
for filename in os.listdir(data_folder):
    if filename.lower().endswith(".pdf"):
        file_path = os.path.join(data_folder, filename)
        print(f"üìò Loading new file: {filename}")
        loader = PyPDFLoader(file_path)
        docs = loader.load()

        for d in docs:
            d.metadata["source"] = filename
            d.metadata["page"] = d.metadata.get("page", "?")

        all_docs.extend(docs)

if not all_docs:
    print("‚ö†Ô∏è No PDF files found in 'data' folder!")
    exit()

# =====================================================
# ‚úÇÔ∏è Split text into manageable chunks
# =====================================================
splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap,
    separators=["\n\n", "\n", ".", " "],
)
chunks = splitter.split_documents(all_docs)

valid_chunks = []
for doc in chunks:
    text = re.sub(r"[^ -~\n]", "", doc.page_content).strip()
    if 50 < len(text) < 3000:
        doc.page_content = text
        valid_chunks.append(doc)

print(f"üß© Split into {len(valid_chunks)} clean chunks.")

# =====================================================
# üß† Build / update Chroma database
# =====================================================
print("‚öôÔ∏è Adding new documents to Chroma database...")
db = Chroma(
    persist_directory=persist_dir,
    embedding_function=embeddings,
)
db.add_documents(valid_chunks)

print(f"‚úÖ Successfully added {len(valid_chunks)} chunks to '{persist_dir}'")
print("üîó Metadata (file + page) successfully embedded ‚Äî ready for search.")
